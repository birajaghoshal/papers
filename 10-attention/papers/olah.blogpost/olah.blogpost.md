## Blogpost
  - **Title:** Attentional Interfaces
  - **Authors:** Christopher Olah
  - **Year:** 2016
  - **Link:** https://distill.pub/2016/augmented-rnns/#attentional-interfaces
  - **Intro Paragraph:** When I’m translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so. Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they’re given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN.
